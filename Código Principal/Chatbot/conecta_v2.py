# -*- coding: utf-8 -*-
"""conecta_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZY0xOfqvabYHEETQgbRNMZUnE6_HOnxb
"""

!pip install -U langchain-huggingface langchain-mongodb pymongo fpdf bitsandbytes pypdf langchain-community hf_xet

!pip install "transformers>=4.45.1"

from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
import torch
from pymongo import MongoClient
import datetime
import os
from google.colab import userdata
import gc
from langchain.prompts import PromptTemplate
import re

# Set Hugging Face token
os.environ["HUGGINGFACEHUB_API_TOKEN"] = userdata.get("HF_TOKEN")

os.environ["LANGCHAIN_API_KEY"] = "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"
os.environ["LANGCHAIN_TRACING_V2"] = "true"      # Ativa o logging

# Check GPU availability
!nvidia-smi

#Configuração do Llama-2 8-bit (reduz RAM em ~50%)
bnb_config = BitsAndBytesConfig(
  load_in_8bit=True,
  bnb_8bit_quant_type="nf4",  # Mais estável que nf8
  bnb_8bit_compute_dtype=torch.float16,
  bnb_8bit_use_double_quant=True  # Reduz ainda mais o uso de memória
)

#1. Carrega o modelo quantizado
model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name, token=userdata.get("HF_TOKEN"))
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="cuda",
    token=userdata.get("HF_TOKEN"),
    torch_dtype=torch.float16,  # Otimização adicional
    low_cpu_mem_usage=True
)

# 2. Cria o pipeline com otimizações
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.7,
    do_sample=True,
    device_map="auto"
)
llm = HuggingFacePipeline(pipeline=pipe)

# 3. Configure o MongoDB (como no seu código original)
client = MongoClient("mongodb+srv://conecta-ia:O1r3VIK4X35CzEfL@conecta-cluster.hgjlsdc.mongodb.net/")
db = client["conecta"]

class ProcessamentoDeDocumento:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cuda'}
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100
        )

    def process_pdf(self, file_path, user_id):
        try:
            loader = PyPDFLoader(file_path)
            pages = loader.load()
            chunks = self.text_splitter.split_documents(pages)
            for chunk in chunks:
                chunk.metadata["user_id"] = user_id

            doc_id = db.documents.insert_one({
                "user_id": user_id,
                "original_path": file_path
            }).inserted_id

            MongoDBAtlasVectorSearch.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                collection=db.document_vectors,
                index_name="document_search"
            )
            gc.collect()
            return doc_id
        except Exception as e:
            print(f"Erro: {str(e)}")
            return None

class QASystem:
    def __init__(self):
        self.llm = llm  # Usando o Llama-2 configurado acima
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cuda'}
        )
        self.vector_store = MongoDBAtlasVectorSearch(
            collection=db.document_vectors,
            embedding=self.embeddings,
            index_name="document_search"
        )

    def ask_question(self, question, user_id):
        try:
            retriever = self.vector_store.as_retriever(
             filter={"user_id": user_id},
             search_kwargs={"k": 5}
            )
            # print("\n--- DEBUG: Documentos Recuperados ---")
            # retrieved_docs = retriever.invoke(question) # Usa invoke para ver os documentos
            # if retrieved_docs:
            #     print(f"Número de documentos encontrados: {len(retrieved_docs)}")
            #     for i, doc in enumerate(retrieved_docs):
            #         print(f"  Documento {i+1} (Fonte: {doc.metadata.get('source', 'N/A')}):")
            #         print(f"    Conteúdo (primeiros 200 chars): {doc.page_content[:200]}...")
            # else:
            #     print("Nenhum documento relevante encontrado pelo retriever.")
            # print("--- FIM DEBUG ---")
            template = """Com base **apenas** no contexto fornecido, responda à pergunta **em português**.
                        Formule uma resposta **clara, concisa e natural**, sem introduções, o contexto ou a pergunta.
                        Se a resposta não puder ser encontrada no contexto fornecido, responda **apenas**: "Não consegui encontrar a resposta para esta pergunta no documento fornecido."

            Contexto:
            {context}

            Pergunta: {question}

            Resposta:""" # Mantemos "Resposta:" aqui para delimitar a saída do modelo

            PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

            qa = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={"prompt": PROMPT}
            )

            result = qa.invoke({"query": question})

            resposta_bruta = result["result"].strip()
            resposta_limpa = resposta_bruta # Começa com a resposta bruta

            # 1. Remoção agressiva do prompt completo, priorizando a parte "Resposta:"
            # O texto do prompt que você está vendo vazando é:
            # "Com base **apenas** no contexto fornecido, responda à pergunta **em português**. Formule uma resposta **clara, concisa e natural**, sem introduções, o contexto ou a pergunta. Se a resposta não puder ser encontrada no contexto fornecido, responda **apenas**: "Não consegui encontrar a resposta para esta pergunta no documento fornecido.""

            # Vamos procurar pela última ocorrência da sua própria instrução "Resposta:" e pegar tudo que vem DEPOIS dela.
            # Isso é o mais eficaz se o modelo estiver ecoando o prompt.
            end_marker_prompt_part = "Resposta:"
            if end_marker_prompt_part in resposta_limpa:
                resposta_limpa = resposta_limpa.split(end_marker_prompt_part, 1)[-1].strip()

            # Em alguns casos, o prompt pode começar com "Com base..." e terminar com "Resposta:",
            # se a linha acima não pegou, tentamos uma remoção mais genérica
            # Remover o texto completo do prompt, caso ele tenha sido replicado
            full_prompt_text_start = "Com base **apenas** no contexto fornecido, responda à pergunta **em português**."
            full_prompt_text_fallback = "Com base **exclusivamente** no contexto fornecido, responda à seguinte pergunta."


            if resposta_limpa.startswith(full_prompt_text_start):
                resposta_limpa = resposta_limpa.replace(full_prompt_text_start, "", 1).strip()
            elif resposta_limpa.startswith(full_prompt_text_fallback):
                resposta_limpa = resposta_limpa.replace(full_prompt_text_fallback, "", 1).strip()


            # 2. Remoção de resquícios comuns do Llama-2 chat e marcadores de contexto/pergunta
            if resposta_limpa.lower().startswith("helpful answer:"): # Usar .lower() para ser case-insensitive
                resposta_limpa = resposta_limpa[len("helpful answer:"):].strip()
            if resposta_limpa.lower().startswith("a resposta é:"):
                resposta_limpa = resposta_limpa[len("a resposta é:"):].strip()
            if resposta_limpa.lower().startswith("here's the answer:"):
                resposta_limpa = resposta_limpa[len("here's the answer:"):].strip()

            # Remover qualquer marcador de contexto/pergunta que possa ter vazado (mais comum)
            # Esses marcadores podem ter sido parte do prompt que o modelo ecoou.
            if "Contexto:" in resposta_limpa:
                resposta_limpa = resposta_limpa.split("Contexto:", 1)[0].strip()
            if "Pergunta:" in resposta_limpa:
                resposta_limpa = resposta_limpa.split("Pergunta:", 1)[0].strip()
            if "Resposta:" in resposta_limpa: # Remove "Resposta:" que pode ter sobrado se não foi o último marcador
                resposta_limpa = resposta_limpa.replace("Resposta:", "").strip()


            # 3. Limpeza de caracteres estranhos e formatação
            # Remover padrões de erro de OCR como "vir/t.alt/u.alt/a.altl" ou "t.alt/u.alt/a.altl"
            # O padrão é algo como /letra.alt/letra.alt/letra.altl ou similar
            resposta_limpa = re.sub(r'\S*/[a-zA-Z]\.alt(/[a-zA-Z]\.alt)*l?', '', resposta_limpa)
            resposta_limpa = re.sub(r'\s{2,}', ' ', resposta_limpa).strip() # Remove múltiplos espaços em branco
            resposta_limpa = re.sub(r'\s*([.,;?!])', r'\1', resposta_limpa) # Remove espaços antes de pontuação
            resposta_limpa = re.sub(r'([.,;?!])\s*(?=[a-zA-Z0-9])', r'\1 ', resposta_limpa) # Garante espaço depois de pontuação
            resposta_limpa = resposta_limpa.replace('**', '') # Remove negrito, se não quiser que apareça na resposta final do modelo

            # Se a resposta ainda estiver muito poluída e não for a frase de "não consegui",
            # podemos tentar pegar apenas a primeira frase ou linha para uma resposta mais segura.
            # Isso é um último recurso para ter algo útil e limpo.
            # if 'Não consegui encontrar a resposta' not in resposta_limpa and '\n' in resposta_limpa:
            #     resposta_limpa = resposta_limpa.split('\n', 1)[0].strip()


            fontes_unicas = list(set([doc.metadata.get("source", doc.metadata.get("file_name", "Desconhecido")) for doc in result["source_documents"]]))

            return {
                "resposta": resposta_limpa,
                "fontes": fontes_unicas
            }
        except Exception as e:
            print(f"Erro ao responder a pergunta '{question}': {str(e)}")
            return None

if __name__ == "__main__":
    processor = ProcessamentoDeDocumento()
    qa = QASystem()

    user_id = "12345"
    doc_id = processor.process_pdf("/content/ebook_como_vender_marketplace.pdf", user_id)

    if doc_id:
        resposta = qa.ask_question("O que é um marketplace?", "123")
        print(resposta)